\subsection{NIST SP 800-90B}

The NIST Special Publication 800-90B is the second in a series of publications regarding the design and implementation
of random bit generators (RBGs). Whereas SPs 800-90A and 800-90C describe the design and validation of deterministic 
random bit generators and their integration with an entropy source, SP 800-90B provides specification for the design
and validation of entropy sources. 

Central to the recommendation set forth by NIST SP 800-90B is the measurement of entropy; specifically the publication
focuses on min-entropy, and then the determination of the entropy source's ability to generate independent and 
identically distributed entropy samples. 

Furthermore, the publication sets out a block diagram model of an expected design of an entropy source, which includes
the following components:

\begin{enumerate}
    \item An analog noise source.
    \item A digitization mechanism to convert the noise source's output into bits.
    \item Built-in health tests with error reporting.
    \item An optional conditioning mechanism.
\end{enumerate}

The publication additionally describes a high-level functional interface that specify several example interactions that
the system can perform. These are not given as requirements, since entropy sources may vary in implementation.

\begin{enumerate}
    \item \textbf{GetEntropy} - a mechanism to retrieve an amount of entropy.
    \item \textbf{GetNoise} - a mechanism to retrieve raw samples of the noise source for validation 
        or health testing.
    \item \textbf{HealthTest} - a mechanism to request that the entropy source perform health tests.
\end{enumerate}

\subsubsection{Data Collection and Validation}

The publication specifies three requirements regarding data collection, first two are mutually exclusive depending on 
the inclusion of a conditioning component, but essentially require the collection of 1 000 000 samples for validation.
The third requirement specifies that for the restart tests, 1000 consecutive samples must be collected, 1000 times. 
This allows for testing for undesired correlation between ideally unique rows and columns.

The specification further breaks down the validation testing into the \textit{independent and identically distributed} 
(IID) path and the non-IID path. For an entropy source to be independent and identically distributed, the symbols it
generates need to be distributed according to a uniform distribution, that is, no symbol can be more commonly occurring
than any other, and each symbol must be independent of any other symbol.

\subsubsection{Determining Validation Pathway}

In order to determine the correct path to take for validation, the noise source must be tested to see whether the IID
assumption holds. Several tests are provided and are categorized into two categories, namely permutation testing and 
additional chi-square statistical tests. Permutation testing is a means to test a statistical hypothesis by comparing
the actual value of a test statistic against inferred reference distributions generated by the input data. 
Additionally, the chi-square tests are provided in order to discover dependencies in the probabilities between
successive samples or discover discrepancies between the distributions of different subsets of the dataset and the
dataset as a whole.

The permutation tests are listed as follows:

\begin{enumerate}
    \item Excursion Test Statistic
    \item Number of Directional Runs
    \item Length of Directional Runs
    \item Number of Increases and Decreases
    \item Number of Runs based on the Median
    \item Length of Runs based on the Median
    \item Average Collision Test Statistic
    \item Periodicity Test Statistic
    \item Covariance Test Statistic
    \item Compression Test Statistic
\end{enumerate}

% Add notes on each test here as they are implemented

The independence and goodness-of-fit tests depend on the type of input data, as the mechanisms change based on whether
the range of outputs from a given noise source is binary or non-binary.

\subsubsection{Validation Tests}

The primary metric the NIST SP 800-90B tests estimate is the min-entropy metric, which fundamentally describes how 
difficult it is for an adversary to predict the output of a given entropy source. As a result, several tests are 
provided depending on whether the IID assumption has been shown to be valid or not. If the samples a noise source
produces is shown to be independent and identically distributed, then the number of occurrences of each possible 
symbol should be roughly equivalent in a given dataset. However, if the noise source is shown to not be IID, then some
types of validation tests will give overconfident estimates of min-entropy. 

The min-entropy estimators provided are listed as follows:

\begin{enumerate}
    \item \textbf{Most Common Value Estimate}
    \item \textbf{Collision Estimate (binary inputs only)}
    \item \textbf{Markov Estimate (binary inputs only)}
    \item \textbf{Compression Estimate (binary inputs only)}
    \item \textbf{\textit{t}-Tuple Estimate}
    \item \textbf{Longest Repeated Substring (LRS) Estimate}
    \item \textbf{Multi Most Common in Window (MultiMCW) Prediction Estimate}
    \item \textbf{Lag Prediction Estimate}
    \item \textbf{Multiple Markov Model with Counting (MultiMMC) Prediction Estimate}
    \item \textbf{LZ78Y Prediction Estimate}
\end{enumerate}

Each test is implemented with a specific scenario in mind. For IID entropy sources, the Most Common Value estimate 
is sufficient; however, for non-IID entropy sources, it gives an overestimate of min-entropy, and as such the 
remaining nine tests are required to determine the minimum estimate. The lowest min-entropy estimate is determined
as the min-entropy for the entropy source, as a guarantee that the entropy source has at maximum this entropy and 
no greater.

Certain min-entropy estimators are designated as for binary inputs only, i.e., that the entropy source only generates
two symbols (typically termed '1' and '0') rather than a larger set of symbols. Binary inputs have the smallest legal
symbol set, as a symbol set of size 1 is inherently deterministic and thus fails as an entropy source, but the 
Collision Estimate, Markov Estimate, and Compression Estimate make use of binary properties and probabilities (e.g., 
$P_{0\rightarrow0}$, $P_{0\rightarrow1}$, and vice versa) to identify biases in the entropy samples.

The remaining six tests also work to identify biases but in the form of sequences (namely the \textit{t}-Tuple
Estimate, LRS Estimate, MultiMCW Prediction Estimate), in the form of repetition after a certain lag time (the aptly-
named Lag Prediction Estimate), or in the form of predicting transitions or of sequences (the LZ78Y Prediction
Estimate). 

\subsubsection{Health Tests}

% Note: revisit health tests here